---
title: Backward
date: 2023-03-13-22:13:00 +0900
categories: [AI, ML]
tags: [PyTorch, Backward, Autograd]
---

### ✔️ 간단 요약
> Forward의 결과값(model의 output=예측치)과 실제값간의 차이(loss)에 대해 미분을 수행한다.
> 
> 미분 수식을 따로 작성하지 않아도, 자동으로 미분이 가능하다.(Autograd)
> 
> 해당 값으로 Parameter를 갱신한다.  
{: .prompt-info }  
> Autograd, loss, optimizer, nn.Module  
---

```python
for epoch in range(epochs):
# … …
# 이전 epoch에서 발생한 기울기값이 현재의 기울기값에 영향을 미치는 것을 방지하기 위해
# gradient buffer을 0으로 초기화해준다.
**optimizer.zero_grad()**

# 모델에 input을 넣고 output을 반환받는다.
**outputs = model(inputs)**

# label(y)과 output(^y)을 비교하여 loss값을 계산한다.
loss = criterion(outputs, labels) 
print(loss)

# loss에 대해 구하고자 하는 모든 weight값을 구해준다.
loss.backward()

# 파라미터를 갱신한다.
optimizer.step()
# ………
```

실제 backward는 Module 단계에서 직접 지정이 가능하다.

Module에서 backward와 optimizer를 오버라이딩 해주면 된다.

사용자가 직접 미분 수식을 써야하는 부담이 있다.

쓸 일은 없지만, 순서를 이해할 필요는 있다.

### 예제 — Logistic Regression

```python
class LR(nn.Module):
		def init (self, dim, lr=torch.scalar_tensor(0.01)):
				super(LR, self). init () 
				# intialize parameters
				self.w = torch.zeros(dim, 1, dtype=torch.float).to(device)
				self.b = torch.scalar_tensor(0).to(device)
				self.grads = {"dw": torch.zeros(dim, 1, dtype=torch.float).to(device), 
											"db": torch.scalar_tensor(0).to(device)}
				self.lr = lr.to(device)
```

$$
h_\theta(x)=\frac{1}{1+e^{-\theta^T \mathbf{x}}}
$$

```python

		def forward(self, x): 
				## compute forward
				z = torch.mm(self.w.T, x) 
				a = self.sigmoid(z) 
				return a
		
		def sigmoid(self, z):
				return 1/ (1 + torch.exp(-z))

```

```python

		def backward(self, x, yhat, y): 
				## compute backward
				self.grads["dw"] = (1/x.shape[1]) * torch.mm(x, (yhat - y).T) 
				self.grads["db"] = (1/x.shape[1]) * torch.sum(yhat - y)

```

$$
\begin{aligned}&\frac{\partial}{\partial \theta_j} J(\theta)=\frac{1}{m} \sum_{i=1}^m\left(h_\theta\left(x^i\right)-y^i\right) x_j^i\end{aligned}
$$

```python

		def optimize(self):
				## optimization step
				self.w = self.w - self.lr * self.grads["dw"]
				self.b = self.b - self.lr * self.grads["db"]
```

기존의 $\theta$, 즉, $\tt w$ 값에 미분값 만큼의 업데이트를 수행해주는 함수. 

$$
\begin{aligned}\theta_j & :=\theta_j-\alpha \frac{\partial}{\partial \theta_j} J(\theta) \\& :=\theta_j-\alpha \sum_{i=1}^m\left(h_\theta\left(x^i\right)-y^i\right) x_j^i\end{aligned}
$$

- 전체 코드
    
    ```python
    class LR(nn.Module):
    		def init (self, dim, lr=torch.scalar_tensor(0.01)):
    				super(LR, self). init () 
    				# intialize parameters
    				self.w = torch.zeros(dim, 1, dtype=torch.float).to(device)
    				self.b = torch.scalar_tensor(0).to(device)
    				self.grads = {"dw": torch.zeros(dim, 1, dtype=torch.float).to(device), 
    											"db": torch.scalar_tensor(0).to(device)}
    				self.lr = lr.to(device)
    
    		def forward(self, x): 
    				## compute forward
    				z = torch.mm(self.w.T, x) 
    				a = self.sigmoid(z) 
    				return a
    		
    		def sigmoid(self, z):
    				return 1/ (1 + torch.exp(-z))
    
    		def backward(self, x, yhat, y): 
    				## compute backward
    				self.grads["dw"] = (1/x.shape[1]) * torch.mm(x, (yhat - y).T) 
    				self.grads["db"] = (1/x.shape[1]) * torch.sum(yhat - y)
    
    		def optimize(self):
    				## optimization step
    				self.w = self.w - self.lr * self.grads["dw"]
    				self.b = self.b - self.lr * self.grads["db"]
    ```